{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02. 선형회귀 연습",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOd7ZTqc8Pg2nfYWTbksf1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyunsooooo/Pytorch-/blob/main/02_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80_%EC%97%B0%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Eeuva_XyFBh"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRYf84SWyHgM"
      },
      "source": [
        "# 선형회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvuXkOc3yUB8"
      },
      "source": [
        "## 선형회귀\r\n",
        "선형 회귀 이론에 대해 이해하고, pytorch를 이용하여 선형 회귀 모델을 만들어 보겠습니다.\r\n",
        "- Data Definition(데이터에 대한 이해)\r\n",
        "> 학습할 데이터에 대해\r\n",
        "\r\n",
        "- Hypothesis(가설) 수립\r\n",
        "> 가설을 수립하는 방법에 대해\r\n",
        "\r\n",
        "- Comput loss(손실 계산)\r\n",
        "> 학습데이터를 이용해서 연속적으로 모델을 개선시키는데 이 때의 loss를 이용\r\n",
        "\r\n",
        "- Gradient Descent(경사하강법)\r\n",
        "> 학습을 위한 핵심 알고리즘인 경사하강법에 대해 이해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFP2m-14zZah"
      },
      "source": [
        "### Data Definition\r\n",
        "공부한 시간과 점수에 대한 상관관계."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJAvDVrT0OvX"
      },
      "source": [
        "1) Training Dataset , Test Dataset\r\n",
        "\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/data_definition.PNG)\r\n",
        "\r\n",
        "어떤 학생이 1시간 공부를 했더니 2점, 다른 학생이 2시간 공부를 했더니 4점, 또 다른 학생이 3시간을 공부했더니 6점을 맞았습니다. 그렇다면, ***내가 4시간을 공부한다면 몇 점을 맞을 수 있을까요?***\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/linear_regression.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhp2a5uZ0P41"
      },
      "source": [
        "2) 훈련 데이터셋의 구성\r\n",
        "\r\n",
        "앞서 텐서에 대해 배웠는데, 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있어야 한다. 그리고 입력과 출력을 각기 다른 텐서에 저장할 필요가 있다. 이 때 보편적으로 입력x 출력y 로 표기한다.\r\n",
        "\r\n",
        "여기서 x_train은 공부한 시간, y_train은 그에 맵핑되는 점수를 의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZETCciy0gHt"
      },
      "source": [
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoXYIkd_0tRH"
      },
      "source": [
        "![대체 텍스트](https://wikidocs.net/images/page/53560/tensor1.PNG)\r\n",
        "\r\n",
        "이제 모델의 가설을 세워보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w8IlKVQ0whB"
      },
      "source": [
        "### 가설의 수립"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I10pZEMc00vW"
      },
      "source": [
        "머신 러닝에서 식을 세울 때 이 식을 가설이라 한다. 보통 머신 러닝에서 가설은 임의로 추측해서 세워보는 식일 수도 있고, 경험적으로 알고 있는 식일 수도 있다. 그리고 맞는 가설이 아니라고 판단되면 계속 수정해나가는 식이기도 하다.\r\n",
        "\r\n",
        "선형 회귀의 가설은 이미 널리 알려져있으므로 고민이 필요 없다. 선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일이다. 이 때 선형 회귀의 가설(직선의 방정식)은 다음과 같은 형식을 가진다.\r\n",
        "$y=Wx+b$\r\n",
        "\r\n",
        "가설의 H를 따서 y대신 다음과 같이 표현하기도 한다.\r\n",
        "\r\n",
        "$H(x)=Wx+b$\r\n",
        "\r\n",
        "이때 $x$와 곱해지는 $W$를 가중치(Weight)라고 하며, $b$를 편향(bias)이라고 한다.\r\n",
        "\r\n",
        "*   $W$ 와 $b$는 중학교 수학 과정인 직선의 방정식에서 기울기와 y절편에 해당된다.\r\n",
        "*   직선의 방정식 링크 : https://mathbang.net/443\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA7jJJtq1WUB"
      },
      "source": [
        "### 비용함수(cost function)에 대한 이해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPjSIcq1q9Z"
      },
      "source": [
        "**비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9OPF5O510Zn"
      },
      "source": [
        "비용함수에 대한 이해를 돕기 위한 예제\r\n",
        "4개의 훈련데이터와 이를 2차원 그래프에 4개의 점으로 표현한 상태\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC1.PNG)\r\n",
        "\r\n",
        "목표는 4개의 점을 가장 잘 표현하는 직선을 그리는 것 다음 3개의 직선 중 가장 잘 표현한 것은?\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC2.PNG)\r\n",
        "\r\n",
        "검은색 직선의 4개의 점에 가장 가깝다는 느낌이 든다.\r\n",
        "-> 이를 수학적으로 표현하기 위해 error(오차)라는 개념을 도입\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC3.PNG)\r\n",
        "\r\n",
        "위 그림은 실제값과 예측값에 대한 차이를 빨간색 화살표로 표현한 것. 이를 error라고 할 수 있다.\r\n",
        "\r\n",
        "\r\n",
        "|$hours(x)$|2|3|4|5|\r\n",
        "|:-|:-|:-|:-|:-|\r\n",
        "|실제값|25|50|42|61|\r\n",
        "|예측값|27|40|53|66|\r\n",
        "|오차|-2|10|-11|-5|\r\n",
        "\r\n",
        "\r\n",
        "총 오차(total error)를 구할 때에는 각 오차를 제곱한 뒤 전부 더해준다\r\n",
        "\r\n",
        "*제곱하지 않고 더하면 음수와 양수를 더하게 되므로 제대로 된 오차의 크기를 측정할 수 없다.*\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCZW1bHM4Hyw"
      },
      "source": [
        "이를 수식으로 표현하게 되면\r\n",
        "\r\n",
        "$\\sum_{i=1}^{n}[y^{(i)} - H(x^{(i)})]^{2} = (-2)^{2}+(10)^{2}+(-11)^{2}+(-5)^{2} = 250$\r\n",
        "\r\n",
        "여기서 n은 갖고 있는 데이터의 크기.\r\n",
        "\r\n",
        "오차의 제곱합을 n으로 나누면 평균 제곱 오차(Mean squered Error, MSE)\r\n",
        "\r\n",
        "$\\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)} - H(x^{(i)})]^{2} = 250/4 = 62.5$\r\n",
        "\r\n",
        "평균 제곱 오차를 $W$ 와 $b$에 의한 비용 함수(Cost function)로 재정의해보면\r\n",
        "\r\n",
        "$cost(W,b) = \\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)} - H(x^{(i)})]^{2}$\r\n",
        "\r\n",
        "$Cost(W,b)$를 최소가 되게 만드는 $W$와 $b$를 구하면 훈련 데이터를 가장 잘 나타내는 직선을 구할 수 있다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HconAzK-0EIw"
      },
      "source": [
        "### 옵티마이저 - 경사 하강법(Gradient Descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV4se7fj5Ekk"
      },
      "source": [
        "앞서 정의한 비용 함수(Cost Function)의 값을 최소로 하는 W와 b를 찾는 방법에 대해. 이때 사용되는 것이 옵티마이저 알고리즘. 최적화 알고리즘이라고도 함. 그리고 이 옵티마이저 알고리즘을 통해 적절한 W와 b를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 부름. 여기서 가장 기본적인 옵티마이저 알고리즘인 경사하강법에 대해.\r\n",
        "\r\n",
        "편향 b에 대해 고려하지 않은 b=0 인 y = Wx와 같은 식을 기준으로 설명.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC4.PNG)\r\n",
        "\r\n",
        "가중치 $W$가 직선의 방정식에서는 기울기였음을 기억. 이제 $W$를 기울기라고 명명하고 설명.\r\n",
        "\r\n",
        "위의 그림에서 주황색선은 기울기 $W$가 20일 때, 초록색선은 기울기 $W$가 1일 때를 보여줌.↕는 각 점에서의 실제값과 두 직선의 예측값과의 오차를 보여줌. 이는 앞서 예측에 사용했던 $y=13x+1$ 직선보다 확연히 큰 오차값들. 즉, 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커짐.\r\n",
        "\r\n",
        "설명의 편의를 위해 편향 $b$가 없이 단순히 가중치 $W$만을 사용한 $H(x)=Wx$라는 가설을 가지고, 경사 하강법을 설명. 비용 함수의 값 $cost(W)$는 cost라고 줄여서 표현. 이에 따라 $W$와 cost의 관계를 그래프로 표현하면.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%99%80%EC%BD%94%EC%8A%A4%ED%8A%B8.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtXsrHs_6OVE"
      },
      "source": [
        "기계는 임의의 초기값 W값을 정한뒤, 맨 아래 볼록한 부분을 향해 점차 W의 값을 수정. 그리고 이를 가능하게 하는 것이 경사 하강법.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG)\r\n",
        "\r\n",
        "초록색 선은 W가 임의의 값을 가지는 네 가지 경우에 대해서, 그래프 상의 접선의 기울기를 보여줌. 여기서 맨 아래로 갈 수록 접선의 기울기가 점차 작아짐. 맨 아래의 볼록한 부분에서는 0 이 됨.\r\n",
        "\r\n",
        "즉, cost의 최소 지점은 접선의 기울기가 0이 되는 지점이며, 미분값이 0이 되는 지점. 경사 하강법의 아이디어는 비용 함수를 미분하여 현재 W에서의 접선의 기울기를 구하고 접선의 기울기가 낮은 방향으로 W의 값을 변경하는 작업을 반복.\r\n",
        "\r\n",
        "이 반복 작업에는 현재 W의 접선의 기울기를 구해 특정 숫자 α를 곱한 값을 빼서 새로운 W로 사용한다.\r\n",
        "\r\n",
        "기울기$ = \\frac{\\partial cost(W)}{\\partial W}$\r\n",
        "\r\n",
        "*   기울기가 음수일 때 : $W$의 값이 증가\r\n",
        "> W:=W−α×(음수기울기)=W+α×(양수기울기)\r\n",
        "\r\n",
        "\r\n",
        "*   기울기가 양수일 때 : W의 값이 감소\r\n",
        "> W:=W−α×(양수기울기)\r\n",
        "\r\n",
        "W:=W−α$\\frac{\\partial }{\\partial W}cost(W)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT7GIKK98CdH"
      },
      "source": [
        "여기서 학습률(learning rate)라 하는 α는 어떤 의미인가. α는 W의 값을 변경할 때, 얼마나 크게 변경할지를 정함. 또는 W를 그래프의 한 점으로 보고 접선의 기울기가 0이 될 때 까지 경사를 따라 내려간다는 관점으로 보면, 얼마나 큰 폭으로 이동할 지를 정함. 학습률 α를 무작정 크게 하면 금방 접선의 기울기가 최소가 되는 W 값을 찾을 것 같지만 그렇지 않다.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EB%B0%9C%EC%82%B0.PNG)\r\n",
        "\r\n",
        "위의 그림은 학습률 α가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 W값을 찾는 게 아니라 W값이 발산하는 상황을 보여준다. 반대로 α가 너무 낮으면 학습 속도가 느려지므로 적당한 α 값을 차즌ㄴ 것이 중요하다.\r\n",
        "\r\n",
        "실제 경사 하강법은 W와 b에 대해서 동시에 경사하강법을 수행하며 최적의 W와 b 값을 찾아간다.\r\n",
        "\r\n",
        "- 가설, 비용함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념.. 풀고자 하는 문제에 따라 가설, 비용함수, 옵티마이저는 전부 다를 수 있으며 선형 회귀에 가장 적합한 비용 함수는 평균 제곱 오차, 옵티마이저는 경사 하강법임.\r\n",
        "\r\n",
        "이제 파이토치로 구현하기.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgLYLYdx7mEy"
      },
      "source": [
        "### 파이토치로 선형 회귀 구현하기\r\n",
        "실습을 위해 파이토치의 도구들을 import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2qPo4Ap7qbv"
      },
      "source": [
        "1) 기본 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGhUdezM7sVP"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6OxurRd7yZZ",
        "outputId": "83cac797-ae57-434c-94b7-895ae0846755"
      },
      "source": [
        "# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드를 준다.\r\n",
        "\r\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbb556ccb58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBaDSry673pc"
      },
      "source": [
        "실습을 위한 기본적인 세팅 끝, 훈련 데이터 x_train , y_train 선언"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYA2LYVt7-H0"
      },
      "source": [
        "2) 변수 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nNDLhe8A-W"
      },
      "source": [
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHzp2HlS8KT0"
      },
      "source": [
        "x_train , y_train 의 크기 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQd1JZWZ8MVR",
        "outputId": "897760fb-9535-4ec4-f11d-5bb9dfd3b254"
      },
      "source": [
        "print(x_train)\r\n",
        "print(x_train.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3vN3WX38R4S",
        "outputId": "37fe7917-0342-4e3d-c3c9-87b29d67f8d1"
      },
      "source": [
        "print(y_train)\r\n",
        "print(y_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.],\n",
            "        [4.],\n",
            "        [6.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTKLhuA_8T1G"
      },
      "source": [
        "3) 가중치와 편향의 초기화\r\n",
        "선형 회귀란 학습 데이터와 가장 잘 맞는 직선을 찾는 일. W와 b로 정의가 된다. 따라서 선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 W와 b를 찾는 것\r\n",
        "\r\n",
        "가중치 W를 0으로 초기화 하고, 값을 출력해봄."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQJDifT68g6W",
        "outputId": "54645930-996c-4a27-a01e-8e60433167ff"
      },
      "source": [
        "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시.\r\n",
        "W = torch.zeros(1, requires_grad=True)\r\n",
        "#가중치 W 출력\r\n",
        "print(W)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxTtCVz08pPb"
      },
      "source": [
        "가중치 W가 0임, 여기서 requires_grad =True 가 인자로 주어짐. 이는 이 변수는 학습을 통해서 계속 값이 변경되는 변수임을 의미.\r\n",
        "\r\n",
        "b도 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkOksbxW80pf",
        "outputId": "4e675166-161f-4089-a1bb-3d63b434d004"
      },
      "source": [
        "b = torch.zeros(1, requires_grad=True)\r\n",
        "print(b)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWCTTtY_833h"
      },
      "source": [
        "현재 가중치 W,b 둘다 0이므로 직선의 방정식은\r\n",
        "\r\n",
        "$y=0*x+0$\r\n",
        "\r\n",
        "지금 상태에선 x에 어떤 값이 들어가도 가설은 0을 예측함\r\n",
        "즉  적절한 W,b 값이 아님\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Oyp8gO9EU5"
      },
      "source": [
        "4) 가설 세우기\r\n",
        "\r\n",
        "파이토치 코드 상으로 직선의 방정식에 해당되는 가설을 선언\r\n",
        "\r\n",
        "$H(x) = Wx+b$\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ95ahZF9Iyi",
        "outputId": "d4308fa6-f56b-4151-efd5-42c3497dc52d"
      },
      "source": [
        "hypothesis= x_train*W +b\r\n",
        "print(hypothesis)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC006P4u9MqT"
      },
      "source": [
        "5) 비용 함수 선언\r\n",
        "파이토치 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언\r\n",
        "\r\n",
        "$cost(W,b) = \\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)} - H(x^{(i)})]^{2}$\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dax4eL_y9VV5",
        "outputId": "b27ed8f3-f403-4177-8da5-b82097d7871e"
      },
      "source": [
        "# 앞서 배운 torch.mean 으로 평균을 구함\r\n",
        "cost = torch.mean((hypothesis - y_train)**2)\r\n",
        "print(cost)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuqcCToX9e3c"
      },
      "source": [
        "6) 경사 하강법 구현하기\r\n",
        "\r\n",
        "SGD 는 경사 하강법의 일종임, lr 는 learning rate를 의미 학습 대상인 W와 b가 SGD 에 입력됨."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjBtfQtd9lvo"
      },
      "source": [
        "optimizer = optim.SGD([W,b], lr = 0.01)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf_GLXRw9qbc"
      },
      "source": [
        "optimizer.zero_grad()를 실행함으로써 미분을 통해 얻은 기울기를 0으로 초기화. 기울기를 초기화해야 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있다. 그 다음 cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산됨. 그 다음 경사 하강법의 최적화 함수 optimizer의 .step() 함수를 호출하여 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 lr 을 곱하여 빼줌으로서 업데이트함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I43zqgho9-9s"
      },
      "source": [
        "# gradient 를 0으로 초기화\r\n",
        "optimizer.zero_grad()\r\n",
        "#비용 함수를 미분하여 gradient 계산\r\n",
        "cost.backward()\r\n",
        "# W와 b를 업데이트\r\n",
        "optimizer.step()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNCtiaIC-HsQ"
      },
      "source": [
        "7) 전체 코"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m13CY9-l-NHR",
        "outputId": "504398be-0988-4009-b93e-8def8381cb3a"
      },
      "source": [
        "# 데이터\r\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\r\n",
        "\r\n",
        "# 모델 초기화\r\n",
        "W = torch.zeros(1, requires_grad=True)\r\n",
        "b = torch.zeros(1, requires_grad=True)\r\n",
        "\r\n",
        "# optimizer 설정\r\n",
        "optimizer = optim.SGD([W,b], lr= 0.01)\r\n",
        "\r\n",
        "nb_epochs = 2000 #경사 하강법의 실행 횟수\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "  # H(x) 계산\r\n",
        "  hypothesis= x_train*W +b\r\n",
        "\r\n",
        "  #cost\r\n",
        "  cost = torch.mean((hypothesis-y_train)**2)\r\n",
        "\r\n",
        "  # cost로 H(x) 개선\r\n",
        "  optimizer.zero_grad()\r\n",
        "  cost.backward()\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "  #100번 마다 로그 출력\r\n",
        "  if epoch % 100 == 0:\r\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\r\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\r\n",
        "        ))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
            "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
            "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
            "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
            "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
            "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
            "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
            "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
            "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
            "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
            "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
            "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
            "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
            "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
            "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
            "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
            "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
            "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSUhSkrD-389"
      },
      "source": [
        "최종 훈련 결과를 보면 최적의 기울지 W 는 2에 가깝고 b는 0에 가깝다 데이터를 보면 실제 정답이 W=2 ,b=0 인걸 보면 거의 근사하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XYMzAgV_HtA"
      },
      "source": [
        "## 자동 미분 (Autograd)\r\n",
        "\r\n",
        "경사 하강법 코드엔 requries_grad=True, backward() 등이 나오는데 파이토치에서 제공하는 자동미분 기능을 수행하는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1mYbyXZ_VcT"
      },
      "source": [
        "### 경사 하강법 리뷰\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Xi_IcC_bDx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PD1A7Ti-2g8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}