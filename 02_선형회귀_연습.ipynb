{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02. 선형회귀 연습",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzgU8Ta7Ct/rZGa7qadHQ1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hyunsooooo/Pytorch-/blob/main/02_%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80_%EC%97%B0%EC%8A%B5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Eeuva_XyFBh"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRYf84SWyHgM"
      },
      "source": [
        "# 선형회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvuXkOc3yUB8"
      },
      "source": [
        "## 선형회귀\r\n",
        "선형 회귀 이론에 대해 이해하고, pytorch를 이용하여 선형 회귀 모델을 만들어 보겠습니다.\r\n",
        "- Data Definition(데이터에 대한 이해)\r\n",
        "> 학습할 데이터에 대해\r\n",
        "\r\n",
        "- Hypothesis(가설) 수립\r\n",
        "> 가설을 수립하는 방법에 대해\r\n",
        "\r\n",
        "- Comput loss(손실 계산)\r\n",
        "> 학습데이터를 이용해서 연속적으로 모델을 개선시키는데 이 때의 loss를 이용\r\n",
        "\r\n",
        "- Gradient Descent(경사하강법)\r\n",
        "> 학습을 위한 핵심 알고리즘인 경사하강법에 대해 이해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFP2m-14zZah"
      },
      "source": [
        "### Data Definition\r\n",
        "공부한 시간과 점수에 대한 상관관계."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJAvDVrT0OvX"
      },
      "source": [
        "1) Training Dataset , Test Dataset\r\n",
        "\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/data_definition.PNG)\r\n",
        "\r\n",
        "어떤 학생이 1시간 공부를 했더니 2점, 다른 학생이 2시간 공부를 했더니 4점, 또 다른 학생이 3시간을 공부했더니 6점을 맞았습니다. 그렇다면, ***내가 4시간을 공부한다면 몇 점을 맞을 수 있을까요?***\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/linear_regression.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zhp2a5uZ0P41"
      },
      "source": [
        "2) 훈련 데이터셋의 구성\r\n",
        "\r\n",
        "앞서 텐서에 대해 배웠는데, 모델을 학습시키기 위한 데이터는 파이토치의 텐서의 형태(torch.tensor)를 가지고 있어야 한다. 그리고 입력과 출력을 각기 다른 텐서에 저장할 필요가 있다. 이 때 보편적으로 입력x 출력y 로 표기한다.\r\n",
        "\r\n",
        "여기서 x_train은 공부한 시간, y_train은 그에 맵핑되는 점수를 의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZETCciy0gHt"
      },
      "source": [
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoXYIkd_0tRH"
      },
      "source": [
        "![대체 텍스트](https://wikidocs.net/images/page/53560/tensor1.PNG)\r\n",
        "\r\n",
        "이제 모델의 가설을 세워보겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w8IlKVQ0whB"
      },
      "source": [
        "### 가설의 수립"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I10pZEMc00vW"
      },
      "source": [
        "머신 러닝에서 식을 세울 때 이 식을 가설이라 한다. 보통 머신 러닝에서 가설은 임의로 추측해서 세워보는 식일 수도 있고, 경험적으로 알고 있는 식일 수도 있다. 그리고 맞는 가설이 아니라고 판단되면 계속 수정해나가는 식이기도 하다.\r\n",
        "\r\n",
        "선형 회귀의 가설은 이미 널리 알려져있으므로 고민이 필요 없다. 선형 회귀란 학습 데이터와 가장 잘 맞는 하나의 직선을 찾는 일이다. 이 때 선형 회귀의 가설(직선의 방정식)은 다음과 같은 형식을 가진다.\r\n",
        "$y=Wx+b$\r\n",
        "\r\n",
        "가설의 H를 따서 y대신 다음과 같이 표현하기도 한다.\r\n",
        "\r\n",
        "$H(x)=Wx+b$\r\n",
        "\r\n",
        "이때 $x$와 곱해지는 $W$를 가중치(Weight)라고 하며, $b$를 편향(bias)이라고 한다.\r\n",
        "\r\n",
        "*   $W$ 와 $b$는 중학교 수학 과정인 직선의 방정식에서 기울기와 y절편에 해당된다.\r\n",
        "*   직선의 방정식 링크 : https://mathbang.net/443\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA7jJJtq1WUB"
      },
      "source": [
        "### 비용함수(cost function)에 대한 이해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtPjSIcq1q9Z"
      },
      "source": [
        "**비용 함수(cost function) = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9OPF5O510Zn"
      },
      "source": [
        "비용함수에 대한 이해를 돕기 위한 예제\r\n",
        "4개의 훈련데이터와 이를 2차원 그래프에 4개의 점으로 표현한 상태\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC1.PNG)\r\n",
        "\r\n",
        "목표는 4개의 점을 가장 잘 표현하는 직선을 그리는 것 다음 3개의 직선 중 가장 잘 표현한 것은?\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC2.PNG)\r\n",
        "\r\n",
        "검은색 직선의 4개의 점에 가장 가깝다는 느낌이 든다.\r\n",
        "-> 이를 수학적으로 표현하기 위해 error(오차)라는 개념을 도입\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC3.PNG)\r\n",
        "\r\n",
        "위 그림은 실제값과 예측값에 대한 차이를 빨간색 화살표로 표현한 것. 이를 error라고 할 수 있다.\r\n",
        "\r\n",
        "\r\n",
        "|$hours(x)$|2|3|4|5|\r\n",
        "|:-|:-|:-|:-|:-|\r\n",
        "|실제값|25|50|42|61|\r\n",
        "|예측값|27|40|53|66|\r\n",
        "|오차|-2|10|-11|-5|\r\n",
        "\r\n",
        "\r\n",
        "총 오차(total error)를 구할 때에는 각 오차를 제곱한 뒤 전부 더해준다\r\n",
        "\r\n",
        "*제곱하지 않고 더하면 음수와 양수를 더하게 되므로 제대로 된 오차의 크기를 측정할 수 없다.*\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCZW1bHM4Hyw"
      },
      "source": [
        "이를 수식으로 표현하게 되면\r\n",
        "\r\n",
        "$\\sum_{i=1}^{n}[y^{(i)} - H(x^{(i)})]^{2} = (-2)^{2}+(10)^{2}+(-11)^{2}+(-5)^{2} = 250$\r\n",
        "\r\n",
        "여기서 n은 갖고 있는 데이터의 크기.\r\n",
        "\r\n",
        "오차의 제곱합을 n으로 나누면 평균 제곱 오차(Mean squered Error, MSE)\r\n",
        "\r\n",
        "$\\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)} - H(x^{(i)})]^{2} = 250/4 = 62.5$\r\n",
        "\r\n",
        "평균 제곱 오차를 $W$ 와 $b$에 의한 비용 함수(Cost function)로 재정의해보면\r\n",
        "\r\n",
        "$cost(W,b) = \\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)} - H(x^{(i)})]^{2}$\r\n",
        "\r\n",
        "$Cost(W,b)$를 최소가 되게 만드는 $W$와 $b$를 구하면 훈련 데이터를 가장 잘 나타내는 직선을 구할 수 있다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HconAzK-0EIw"
      },
      "source": [
        "### 옵티마이저 - 경사 하강법(Gradient Descent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV4se7fj5Ekk"
      },
      "source": [
        "앞서 정의한 비용 함수(Cost Function)의 값을 최소로 하는 W와 b를 찾는 방법에 대해. 이때 사용되는 것이 옵티마이저 알고리즘. 최적화 알고리즘이라고도 함. 그리고 이 옵티마이저 알고리즘을 통해 적절한 W와 b를 찾아내는 과정을 머신 러닝에서 학습(training)이라고 부름. 여기서 가장 기본적인 옵티마이저 알고리즘인 경사하강법에 대해.\r\n",
        "\r\n",
        "편향 b에 대해 고려하지 않은 b=0 인 y = Wx와 같은 식을 기준으로 설명.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/53560/%EA%B7%B8%EB%A6%BC4.PNG)\r\n",
        "\r\n",
        "가중치 $W$가 직선의 방정식에서는 기울기였음을 기억. 이제 $W$를 기울기라고 명명하고 설명.\r\n",
        "\r\n",
        "위의 그림에서 주황색선은 기울기 $W$가 20일 때, 초록색선은 기울기 $W$가 1일 때를 보여줌.↕는 각 점에서의 실제값과 두 직선의 예측값과의 오차를 보여줌. 이는 앞서 예측에 사용했던 $y=13x+1$ 직선보다 확연히 큰 오차값들. 즉, 기울기가 지나치게 크면 실제값과 예측값의 오차가 커지고, 기울기가 지나치게 작아도 실제값과 예측값의 오차가 커짐.\r\n",
        "\r\n",
        "설명의 편의를 위해 편향 $b$가 없이 단순히 가중치 $W$만을 사용한 $H(x)=Wx$라는 가설을 가지고, 경사 하강법을 설명. 비용 함수의 값 $cost(W)$는 cost라고 줄여서 표현. 이에 따라 $W$와 cost의 관계를 그래프로 표현하면.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EC%99%80%EC%BD%94%EC%8A%A4%ED%8A%B8.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtXsrHs_6OVE"
      },
      "source": [
        "기계는 임의의 초기값 W값을 정한뒤, 맨 아래 볼록한 부분을 향해 점차 W의 값을 수정. 그리고 이를 가능하게 하는 것이 경사 하강법.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG)\r\n",
        "\r\n",
        "초록색 선은 W가 임의의 값을 가지는 네 가지 경우에 대해서, 그래프 상의 접선의 기울기를 보여줌. 여기서 맨 아래로 갈 수록 접선의 기울기가 점차 작아짐. 맨 아래의 볼록한 부분에서는 0 이 됨.\r\n",
        "\r\n",
        "즉, cost의 최소 지점은 접선의 기울기가 0이 되는 지점이며, 미분값이 0이 되는 지점. 경사 하강법의 아이디어는 비용 함수를 미분하여 현재 W에서의 접선의 기울기를 구하고 접선의 기울기가 낮은 방향으로 W의 값을 변경하는 작업을 반복.\r\n",
        "\r\n",
        "이 반복 작업에는 현재 W의 접선의 기울기를 구해 특정 숫자 α를 곱한 값을 빼서 새로운 W로 사용한다.\r\n",
        "\r\n",
        "기울기$ = \\frac{\\partial cost(W)}{\\partial W}$\r\n",
        "\r\n",
        "*   기울기가 음수일 때 : $W$의 값이 증가\r\n",
        "> W:=W−α×(음수기울기)=W+α×(양수기울기)\r\n",
        "\r\n",
        "\r\n",
        "*   기울기가 양수일 때 : W의 값이 감소\r\n",
        "> W:=W−α×(양수기울기)\r\n",
        "\r\n",
        "W:=W−α$\\frac{\\partial }{\\partial W}cost(W)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT7GIKK98CdH"
      },
      "source": [
        "여기서 학습률(learning rate)라 하는 α는 어떤 의미인가. α는 W의 값을 변경할 때, 얼마나 크게 변경할지를 정함. 또는 W를 그래프의 한 점으로 보고 접선의 기울기가 0이 될 때 까지 경사를 따라 내려간다는 관점으로 보면, 얼마나 큰 폭으로 이동할 지를 정함. 학습률 α를 무작정 크게 하면 금방 접선의 기울기가 최소가 되는 W 값을 찾을 것 같지만 그렇지 않다.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/21670/%EA%B8%B0%EC%9A%B8%EA%B8%B0%EB%B0%9C%EC%82%B0.PNG)\r\n",
        "\r\n",
        "위의 그림은 학습률 α가 지나치게 높은 값을 가질 때, 접선의 기울기가 0이 되는 W값을 찾는 게 아니라 W값이 발산하는 상황을 보여준다. 반대로 α가 너무 낮으면 학습 속도가 느려지므로 적당한 α 값을 차즌ㄴ 것이 중요하다.\r\n",
        "\r\n",
        "실제 경사 하강법은 W와 b에 대해서 동시에 경사하강법을 수행하며 최적의 W와 b 값을 찾아간다.\r\n",
        "\r\n",
        "- 가설, 비용함수, 옵티마이저는 머신 러닝 분야에서 사용되는 포괄적 개념.. 풀고자 하는 문제에 따라 가설, 비용함수, 옵티마이저는 전부 다를 수 있으며 선형 회귀에 가장 적합한 비용 함수는 평균 제곱 오차, 옵티마이저는 경사 하강법임.\r\n",
        "\r\n",
        "이제 파이토치로 구현하기.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgLYLYdx7mEy"
      },
      "source": [
        "### 파이토치로 선형 회귀 구현하기\r\n",
        "실습을 위해 파이토치의 도구들을 import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2qPo4Ap7qbv"
      },
      "source": [
        "1) 기본 세팅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGhUdezM7sVP"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6OxurRd7yZZ",
        "outputId": "83cac797-ae57-434c-94b7-895ae0846755"
      },
      "source": [
        "# 현재 실습하고 있는 파이썬 코드를 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드를 준다.\r\n",
        "\r\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fbb556ccb58>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBaDSry673pc"
      },
      "source": [
        "실습을 위한 기본적인 세팅 끝, 훈련 데이터 x_train , y_train 선언"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYA2LYVt7-H0"
      },
      "source": [
        "2) 변수 선언"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nNDLhe8A-W"
      },
      "source": [
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHzp2HlS8KT0"
      },
      "source": [
        "x_train , y_train 의 크기 출력"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQd1JZWZ8MVR",
        "outputId": "897760fb-9535-4ec4-f11d-5bb9dfd3b254"
      },
      "source": [
        "print(x_train)\r\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1.],\n",
            "        [2.],\n",
            "        [3.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3vN3WX38R4S",
        "outputId": "37fe7917-0342-4e3d-c3c9-87b29d67f8d1"
      },
      "source": [
        "print(y_train)\r\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[2.],\n",
            "        [4.],\n",
            "        [6.]])\n",
            "torch.Size([3, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTKLhuA_8T1G"
      },
      "source": [
        "3) 가중치와 편향의 초기화\r\n",
        "선형 회귀란 학습 데이터와 가장 잘 맞는 직선을 찾는 일. W와 b로 정의가 된다. 따라서 선형 회귀의 목표는 가장 잘 맞는 직선을 정의하는 W와 b를 찾는 것\r\n",
        "\r\n",
        "가중치 W를 0으로 초기화 하고, 값을 출력해봄."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQJDifT68g6W",
        "outputId": "54645930-996c-4a27-a01e-8e60433167ff"
      },
      "source": [
        "# 가중치 W를 0으로 초기화하고 학습을 통해 값이 변경되는 변수임을 명시.\r\n",
        "W = torch.zeros(1, requires_grad=True)\r\n",
        "#가중치 W 출력\r\n",
        "print(W)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxTtCVz08pPb"
      },
      "source": [
        "가중치 W가 0임, 여기서 requires_grad =True 가 인자로 주어짐. 이는 이 변수는 학습을 통해서 계속 값이 변경되는 변수임을 의미.\r\n",
        "\r\n",
        "b도 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkOksbxW80pf",
        "outputId": "4e675166-161f-4089-a1bb-3d63b434d004"
      },
      "source": [
        "b = torch.zeros(1, requires_grad=True)\r\n",
        "print(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWCTTtY_833h"
      },
      "source": [
        "현재 가중치 W,b 둘다 0이므로 직선의 방정식은\r\n",
        "\r\n",
        "$y=0*x+0$\r\n",
        "\r\n",
        "지금 상태에선 x에 어떤 값이 들어가도 가설은 0을 예측함\r\n",
        "즉  적절한 W,b 값이 아님\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20Oyp8gO9EU5"
      },
      "source": [
        "4) 가설 세우기\r\n",
        "\r\n",
        "파이토치 코드 상으로 직선의 방정식에 해당되는 가설을 선언\r\n",
        "\r\n",
        "$H(x) = Wx+b$\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJ95ahZF9Iyi",
        "outputId": "d4308fa6-f56b-4151-efd5-42c3497dc52d"
      },
      "source": [
        "hypothesis= x_train*W +b\r\n",
        "print(hypothesis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.],\n",
            "        [0.],\n",
            "        [0.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC006P4u9MqT"
      },
      "source": [
        "5) 비용 함수 선언\r\n",
        "파이토치 상으로 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언\r\n",
        "\r\n",
        "$cost(W,b) = \\frac{1}{n}\\sum_{i=1}^{n}[y^{(i)} - H(x^{(i)})]^{2}$\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dax4eL_y9VV5",
        "outputId": "b27ed8f3-f403-4177-8da5-b82097d7871e"
      },
      "source": [
        "# 앞서 배운 torch.mean 으로 평균을 구함\r\n",
        "cost = torch.mean((hypothesis - y_train)**2)\r\n",
        "print(cost)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuqcCToX9e3c"
      },
      "source": [
        "6) 경사 하강법 구현하기\r\n",
        "\r\n",
        "SGD 는 경사 하강법의 일종임, lr 는 learning rate를 의미 학습 대상인 W와 b가 SGD 에 입력됨."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjBtfQtd9lvo"
      },
      "source": [
        "optimizer = optim.SGD([W,b], lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf_GLXRw9qbc"
      },
      "source": [
        "optimizer.zero_grad()를 실행함으로써 미분을 통해 얻은 기울기를 0으로 초기화. 기울기를 초기화해야 새로운 가중치 편향에 대해서 새로운 기울기를 구할 수 있다. 그 다음 cost.backward() 함수를 호출하면 가중치 W와 편향 b에 대한 기울기가 계산됨. 그 다음 경사 하강법의 최적화 함수 optimizer의 .step() 함수를 호출하여 인수로 들어갔던 W와 b에서 리턴되는 변수들의 기울기에 lr 을 곱하여 빼줌으로서 업데이트함."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I43zqgho9-9s"
      },
      "source": [
        "# gradient 를 0으로 초기화\r\n",
        "optimizer.zero_grad()\r\n",
        "#비용 함수를 미분하여 gradient 계산\r\n",
        "cost.backward()\r\n",
        "# W와 b를 업데이트\r\n",
        "optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNCtiaIC-HsQ"
      },
      "source": [
        "7) 전체 코"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m13CY9-l-NHR",
        "outputId": "504398be-0988-4009-b93e-8def8381cb3a"
      },
      "source": [
        "# 데이터\r\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\r\n",
        "\r\n",
        "# 모델 초기화\r\n",
        "W = torch.zeros(1, requires_grad=True)\r\n",
        "b = torch.zeros(1, requires_grad=True)\r\n",
        "\r\n",
        "# optimizer 설정\r\n",
        "optimizer = optim.SGD([W,b], lr= 0.01)\r\n",
        "\r\n",
        "nb_epochs = 2000 #경사 하강법의 실행 횟수\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "  # H(x) 계산\r\n",
        "  hypothesis= x_train*W +b\r\n",
        "\r\n",
        "  #cost\r\n",
        "  cost = torch.mean((hypothesis-y_train)**2)\r\n",
        "\r\n",
        "  # cost로 H(x) 개선\r\n",
        "  optimizer.zero_grad()\r\n",
        "  cost.backward()\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "  #100번 마다 로그 출력\r\n",
        "  if epoch % 100 == 0:\r\n",
        "    print('Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Cost: {:.6f}'.format(\r\n",
        "            epoch, nb_epochs, W.item(), b.item(), cost.item()\r\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 W: 0.187, b: 0.080 Cost: 18.666666\n",
            "Epoch  100/2000 W: 1.746, b: 0.578 Cost: 0.048171\n",
            "Epoch  200/2000 W: 1.800, b: 0.454 Cost: 0.029767\n",
            "Epoch  300/2000 W: 1.843, b: 0.357 Cost: 0.018394\n",
            "Epoch  400/2000 W: 1.876, b: 0.281 Cost: 0.011366\n",
            "Epoch  500/2000 W: 1.903, b: 0.221 Cost: 0.007024\n",
            "Epoch  600/2000 W: 1.924, b: 0.174 Cost: 0.004340\n",
            "Epoch  700/2000 W: 1.940, b: 0.136 Cost: 0.002682\n",
            "Epoch  800/2000 W: 1.953, b: 0.107 Cost: 0.001657\n",
            "Epoch  900/2000 W: 1.963, b: 0.084 Cost: 0.001024\n",
            "Epoch 1000/2000 W: 1.971, b: 0.066 Cost: 0.000633\n",
            "Epoch 1100/2000 W: 1.977, b: 0.052 Cost: 0.000391\n",
            "Epoch 1200/2000 W: 1.982, b: 0.041 Cost: 0.000242\n",
            "Epoch 1300/2000 W: 1.986, b: 0.032 Cost: 0.000149\n",
            "Epoch 1400/2000 W: 1.989, b: 0.025 Cost: 0.000092\n",
            "Epoch 1500/2000 W: 1.991, b: 0.020 Cost: 0.000057\n",
            "Epoch 1600/2000 W: 1.993, b: 0.016 Cost: 0.000035\n",
            "Epoch 1700/2000 W: 1.995, b: 0.012 Cost: 0.000022\n",
            "Epoch 1800/2000 W: 1.996, b: 0.010 Cost: 0.000013\n",
            "Epoch 1900/2000 W: 1.997, b: 0.008 Cost: 0.000008\n",
            "Epoch 2000/2000 W: 1.997, b: 0.006 Cost: 0.000005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSUhSkrD-389"
      },
      "source": [
        "최종 훈련 결과를 보면 최적의 기울지 W 는 2에 가깝고 b는 0에 가깝다 데이터를 보면 실제 정답이 W=2 ,b=0 인걸 보면 거의 근사하다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XYMzAgV_HtA"
      },
      "source": [
        "## 자동 미분 (Autograd)\r\n",
        "\r\n",
        "경사 하강법 코드엔 requries_grad=True, backward() 등이 나오는데 파이토치에서 제공하는 자동미분 기능을 수행하는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1mYbyXZ_VcT"
      },
      "source": [
        "### 경사 하강법 리뷰\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9Xi_IcC_bDx"
      },
      "source": [
        "![대체 텍스트](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG)\r\n",
        "\r\n",
        "경사 하강법은 비용 함수를 미분하여 이 함수의 기울기(gradient)를 구해서 비용이 최소화 되는 방향을 찾아내는 알고리즘\r\n",
        "\r\n",
        "모델이 복잡해질 수록 경사 하강법을 넘파이 등으로 직접 코딩하기 어렵다.\r\n",
        "파이토치에선 이런 수고를 덜기 위해 자동미분(Autograd)를 지원함."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT6MUaxV401N"
      },
      "source": [
        "###자동 미분 실습하기\r\n",
        "\r\n",
        "자동 미분 실습. 임의로 $2w^2+5$ 라는 식을 세워보고 , $w$에 대해 미분해본다.\r\n",
        "값이 2인 임의의 스칼라 텐서 w 선언, 이때 requries_grad 를 True로 설정. 이는 이 텐서에 대한 기울기를 저장하겠다는 의미. 이렇게 하면 w.grad 에 w에 대한 미분값이 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PD1A7Ti-2g8"
      },
      "source": [
        "w = torch.tensor(2.0, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qR8GLYYg5Z7a"
      },
      "source": [
        "#수식\r\n",
        "\r\n",
        "y = w**2\r\n",
        "z= 2*y +5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFvqHCJV52Jg"
      },
      "source": [
        "#해당 수식을 w에 대해 미분해야한다. .backward()를 호출하면 해당 수식의 w에 대한 기울기를 계산한다.\r\n",
        "\r\n",
        "z.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqX3qckr59lK",
        "outputId": "e8aa0ca5-f6f5-4747-cd78-b6a08b412d2b"
      },
      "source": [
        "#이제 w.grad를 출력하면 w가 속한 수식을 w로 미분한 값이 저장된다\r\n",
        "\r\n",
        "print(f'수식을 w로 미분한 값: {w.grad}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식을 w로 미분한 값: 8.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZscU2DF6Fmq"
      },
      "source": [
        "## 다중 선형회귀\r\n",
        "\r\n",
        "앞서 배운 x가 1개인 선형 회귀를 단순선형회귀라 하고, 다수의 x로부터 y를 예측하는 것을 다중선형회귀라 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nRcCKnx9P28"
      },
      "source": [
        "### 데이터에 대한 이해\r\n",
        "\r\n",
        "다음과 같은 훈련 데이터가 있다. 단순 선형 회귀와 다른 점은 독립 변수 x의 갯수가 3개이다.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/54841/%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG)\r\n",
        "\r\n",
        "독립 변수 x가 3개이므로 이를 수식으로 표현하면\r\n",
        "\r\n",
        "$H(x)=w_1x_1+w_2x_2+w_3x_3+b$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO57-P5s9gBH"
      },
      "source": [
        "### 파이토치로 구현하기\r\n",
        "\r\n",
        "훈련 데이터에 대한 선언\r\n",
        "\r\n",
        "$H(X) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3}+b$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Ta7sRId9l6X"
      },
      "source": [
        "# 훈련 데이터\r\n",
        "\r\n",
        "x1_train = torch.FloatTensor([[73],[93],[89],[96],[73]])\r\n",
        "x2_train = torch.FloatTensor([[80],[88],[91],[98],[66]])\r\n",
        "x3_train = torch.FloatTensor([[75],[93],[90],[100],[70]])\r\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1dI5VGIk-Iat"
      },
      "source": [
        "# 가중치 w와 편향 b를 선언한다. w도 3개임\r\n",
        "\r\n",
        "w1 = torch.zeros(1, requires_grad=True)\r\n",
        "w2 = torch.zeros(1, requires_grad=True)\r\n",
        "w3 = torch.zeros(1, requires_grad=True)\r\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgC2UPjr_YJW",
        "outputId": "180ebb08-9daa-4f01-a9fd-cfef9cb764f6"
      },
      "source": [
        "# optimizer 설정\r\n",
        "\r\n",
        "optimizer = optim.SGD([w1,w2,w3,b], lr = 1e-5)\r\n",
        "\r\n",
        "nb_epochs = 1000\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "  # H(x)\r\n",
        "  hypothesis = x1_train*w1 + x2_train*w2 + x3_train*w3  + b\r\n",
        "\r\n",
        "  # cost\r\n",
        "  cost = torch.mean((hypothesis - y_train)**2)\r\n",
        "\r\n",
        "  # cost 로 H(x) 개선\r\n",
        "\r\n",
        "  optimizer.zero_grad()\r\n",
        "  cost.backward()\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "  # 100번마다 로그 출력\r\n",
        "\r\n",
        "  if epoch %100 == 0:\r\n",
        "     print('Epoch {:4d}/{} w1: {:.3f} w2: {:.3f} w3: {:.3f} b: {:.3f} Cost: {:.6f}'.format(\r\n",
        "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\r\n",
        "        ))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/1000 w1: 0.294 w2: 0.294 w3: 0.297 b: 0.003 Cost: 29661.800781\n",
            "Epoch  100/1000 w1: 0.674 w2: 0.661 w3: 0.676 b: 0.008 Cost: 1.563628\n",
            "Epoch  200/1000 w1: 0.679 w2: 0.655 w3: 0.677 b: 0.008 Cost: 1.497595\n",
            "Epoch  300/1000 w1: 0.684 w2: 0.649 w3: 0.677 b: 0.008 Cost: 1.435044\n",
            "Epoch  400/1000 w1: 0.689 w2: 0.643 w3: 0.678 b: 0.008 Cost: 1.375726\n",
            "Epoch  500/1000 w1: 0.694 w2: 0.638 w3: 0.678 b: 0.009 Cost: 1.319507\n",
            "Epoch  600/1000 w1: 0.699 w2: 0.633 w3: 0.679 b: 0.009 Cost: 1.266222\n",
            "Epoch  700/1000 w1: 0.704 w2: 0.627 w3: 0.679 b: 0.009 Cost: 1.215703\n",
            "Epoch  800/1000 w1: 0.709 w2: 0.622 w3: 0.679 b: 0.009 Cost: 1.167810\n",
            "Epoch  900/1000 w1: 0.713 w2: 0.617 w3: 0.680 b: 0.009 Cost: 1.122429\n",
            "Epoch 1000/1000 w1: 0.718 w2: 0.613 w3: 0.680 b: 0.009 Cost: 1.079390\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF906PZkAZnD"
      },
      "source": [
        "### 벡터와 행렬 연산으로 바꾸기 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SODqpBDbAPgn"
      },
      "source": [
        "위의 코드를 개선할 수 있음. x의 갯수가 3개였으니까, x1_train, x2_train, x3_train, w1, w2 ,w3 를 일일히 선언해줬는데 1000개라고 하면 너무 번거롭다. 이를 해결하기 위해 행렬 곱셉 연산(벡터의 내적)을 사용한다.\r\n",
        "- 행렬의 곱셈 과정에서 이루어지는 벡터 연산을 벡터 내적(Dot Product)라고 한다.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/54841/%ED%96%89%EB%A0%AC%EA%B3%B1.PNG)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNY79jI1Dlu5"
      },
      "source": [
        "1) 벡터 연산으로 이해하기\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/54841/%EB%82%B4%EC%A0%81.PNG)\r\n",
        "\r\n",
        "두 벡터를 각각 $X$와 $W$로 표현한다면, 가설은 다음과 같습니다.\r\n",
        "\r\n",
        "$H(X)=XW$\r\n",
        "\r\n",
        "$x$의 개수가 3개였음에도 이제는 $X$와 $W$라는 두 개의 변수로 표현된 것을 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l4w1sJBsDsKm"
      },
      "source": [
        "2) 행렬 연산으로 이해하기\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/54841/%ED%9B%88%EB%A0%A8%EB%8D%B0%EC%9D%B4%ED%84%B0.PNG)\r\n",
        "\r\n",
        "전체 훈련 데이터의 개수를 셀 수 있는 1개의 단위 = sample 이라고 한다. 현재 샘플의 수는 5개이다. 각 샘플에서 y를 결정하게 하는 각각의 독립변수 x 를 특성(feature)이라고 한다.\r\n",
        "현재 특성은 3개이다.\r\n",
        "\r\n",
        "이는 종속 변수 x 들의 수가 (sample * feature) = 15개임을 의미한다.\r\n",
        "\r\n",
        "\r\n",
        "$\\left(\r\n",
        "    \\begin{array}{c}\r\n",
        "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\r\n",
        "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\r\n",
        "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\r\n",
        "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\r\n",
        "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\r\n",
        "    \\end{array}\r\n",
        "  \\right)$\r\n",
        "\r\n",
        "  그리고 여기에 가중치 $w_{1}, w_{2}, w_{3}$을 원소로 하는 벡터를 $W$라 하고 이를 곱해보겠습니다.\r\n",
        "\r\n",
        "$\\left(\r\n",
        "    \\begin{array}{c}\r\n",
        "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\r\n",
        "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\r\n",
        "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\r\n",
        "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\r\n",
        "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\r\n",
        "    \\end{array}\r\n",
        "  \\right)\r\n",
        "\\left(\r\n",
        "    \\begin{array}{c}\r\n",
        "      w_{1} \\\\\r\n",
        "      w_{2} \\\\\r\n",
        "      w_{3} \\\\\r\n",
        "    \\end{array}\r\n",
        "  \\right)\r\n",
        "\\  =\r\n",
        "\\left(\r\n",
        "    \\begin{array}{c}\r\n",
        "      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3}\\ \\\\\r\n",
        "      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3}\\ \\\\\r\n",
        "      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3}\\ \\\\\r\n",
        "      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3}\\ \\\\\r\n",
        "      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3}\\ \\\\\r\n",
        "    \\end{array}\r\n",
        "  \\right)$\r\n",
        "\r\n",
        "  위의 식은 결과적으로 다음과 같습니다.\r\n",
        "\r\n",
        "$H(X) = XW$\r\n",
        "\r\n",
        "이 가설에 각 샘플에 더해지는 편향 $b$를 추가해봅시다. 샘플 수만큼의 차원을 가지는 편향 벡터 $B$를 만들어 더합니다.\r\n",
        "\r\n",
        "$\\left(\r\n",
        "    \\begin{array}{c}\r\n",
        "      x_{11}\\ x_{12}\\ x_{13}\\ \\\\\r\n",
        "      x_{21}\\ x_{22}\\ x_{23}\\ \\\\\r\n",
        "      x_{31}\\ x_{32}\\ x_{33}\\ \\\\\r\n",
        "      x_{41}\\ x_{42}\\ x_{43}\\ \\\\\r\n",
        "      x_{51}\\ x_{52}\\ x_{53}\\ \\\\\r\n",
        "    \\end{array}\r\n",
        "  \\right)\r\n",
        "\\left(\r\n",
        "    \\begin{array}{c}\r\n",
        "      w_{1} \\\\\r\n",
        "      w_{2} \\\\\r\n",
        "      w_{3} \\\\\r\n",
        "    \\end{array}\r\n",
        "  \\right)\r\n",
        "+\r\n",
        "\\left(\r\n",
        "    \\begin{array}{c}\r\n",
        "      b \\\\\r\n",
        "      b \\\\\r\n",
        "      b \\\\\r\n",
        "      b \\\\\r\n",
        "      b \\\\\r\n",
        "    \\end{array}\r\n",
        "  \\right)\r\n",
        " \\ =\r\n",
        "\\left(\r\n",
        "    \\begin{array}{c}\r\n",
        "      x_{11}w_{1}+ x_{12}w_{2}+ x_{13}w_{3} + b\\ \\\\\r\n",
        "      x_{21}w_{1}+ x_{22}w_{2}+ x_{23}w_{3} + b\\ \\\\\r\n",
        "      x_{31}w_{1}+ x_{32}w_{2}+ x_{33}w_{3} + b\\ \\\\\r\n",
        "      x_{41}w_{1}+ x_{42}w_{2}+ x_{43}w_{3} + b\\ \\\\\r\n",
        "      x_{51}w_{1}+ x_{52}w_{2}+ x_{53}w_{3} + b\\ \\\\\r\n",
        "    \\end{array}\r\n",
        "  \\right)$\r\n",
        "\r\n",
        "위의 식은 결과적으로 다음과 같습니다.\r\n",
        "\r\n",
        "$H(X)=XW+B$\r\n",
        "\r\n",
        "결과적으로 전체 훈련 데이터의 가설 연산을 3개의 변수만으로 표현하였습니다.\r\n",
        "이와 같이 벡터와 행렬 연산은 식을 간단하게 해줄 뿐만 아니라 다수의 샘플의 병렬 연산이므로 속도의 이점을 가집니다.\r\n",
        "\r\n",
        "이를 참고로 파이토치로 구현해봅시다.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDNF-AwKJACw"
      },
      "source": [
        "3) 행렬 연산을 고려하여 파이토치로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxGfpqoWJDZ9"
      },
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\r\n",
        "                             [93, 88, 93],\r\n",
        "                             [89, 91 ,90],\r\n",
        "                             [96, 98, 100],\r\n",
        "                             [73, 66, 70]])\r\n",
        "\r\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueuDDN_VJhFw",
        "outputId": "74d7318b-d177-4c7d-9558-1cbd8444e74e"
      },
      "source": [
        "print(x_train.shape)\r\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3])\n",
            "torch.Size([5, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PDdFhAXJk29"
      },
      "source": [
        "# 가중치와 편햔 선언\r\n",
        "\r\n",
        "W = torch.zeros((3,1), requires_grad=True)\r\n",
        "b = torch.zeros(1, requires_grad= True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh816XRQKExY"
      },
      "source": [
        "hypothesis = x_train.matmul(W)+b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z0VweNMKaK7"
      },
      "source": [
        "# 가설을 행렬곱으로 간단히 정의"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9zSDf9PKcuc",
        "outputId": "772fbffb-b784-440a-fba9-ee4d75316776"
      },
      "source": [
        "# 데이터 선언\r\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\r\n",
        "                             [93, 88, 93],\r\n",
        "                             [89, 91, 90],\r\n",
        "                             [96, 98, 100],\r\n",
        "                             [73, 66, 70]])\r\n",
        "\r\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])\r\n",
        "\r\n",
        "# 모델 초기화\r\n",
        "W = torch.zeros((3,1), requires_grad=True)\r\n",
        "b = torch.zeros(1, requires_grad=True)\r\n",
        "\r\n",
        "# optimizer 설정\r\n",
        "optimizer = optim.SGD([W,b], lr = 1e-5)\r\n",
        "\r\n",
        "\r\n",
        "nb_epochs = 20\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "  # H(x)\r\n",
        "  hypothesis = x_train.matmul(W) +b\r\n",
        "\r\n",
        "  #cost\r\n",
        "\r\n",
        "  cost = torch.mean((hypothesis-y_train)**2)\r\n",
        "\r\n",
        "  # cost 로 H(x) 개선\r\n",
        "  optimizer.zero_grad()\r\n",
        "  cost.backward()\r\n",
        "  optimizer.step()\r\n",
        "  print('Epoch {:4d}/{} hypothesis: {} Cost: {:.6f}'.format(\r\n",
        "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\r\n",
        "  ))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 hypothesis: tensor([0., 0., 0., 0., 0.]) Cost: 29661.800781\n",
            "Epoch    1/20 hypothesis: tensor([67.2578, 80.8397, 79.6523, 86.7394, 61.6605]) Cost: 9298.520508\n",
            "Epoch    2/20 hypothesis: tensor([104.9128, 126.0990, 124.2466, 135.3015,  96.1821]) Cost: 2915.712402\n",
            "Epoch    3/20 hypothesis: tensor([125.9942, 151.4381, 149.2133, 162.4896, 115.5097]) Cost: 915.040527\n",
            "Epoch    4/20 hypothesis: tensor([137.7967, 165.6247, 163.1911, 177.7112, 126.3307]) Cost: 287.936096\n",
            "Epoch    5/20 hypothesis: tensor([144.4044, 173.5674, 171.0168, 186.2332, 132.3891]) Cost: 91.371063\n",
            "Epoch    6/20 hypothesis: tensor([148.1035, 178.0143, 175.3980, 191.0042, 135.7812]) Cost: 29.758249\n",
            "Epoch    7/20 hypothesis: tensor([150.1744, 180.5042, 177.8509, 193.6753, 137.6805]) Cost: 10.445267\n",
            "Epoch    8/20 hypothesis: tensor([151.3336, 181.8983, 179.2240, 195.1707, 138.7440]) Cost: 4.391237\n",
            "Epoch    9/20 hypothesis: tensor([151.9824, 182.6789, 179.9928, 196.0079, 139.3396]) Cost: 2.493121\n",
            "Epoch   10/20 hypothesis: tensor([152.3454, 183.1161, 180.4231, 196.4765, 139.6732]) Cost: 1.897688\n",
            "Epoch   11/20 hypothesis: tensor([152.5485, 183.3609, 180.6640, 196.7389, 139.8602]) Cost: 1.710552\n",
            "Epoch   12/20 hypothesis: tensor([152.6620, 183.4982, 180.7988, 196.8857, 139.9651]) Cost: 1.651416\n",
            "Epoch   13/20 hypothesis: tensor([152.7253, 183.5752, 180.8742, 196.9678, 140.0240]) Cost: 1.632369\n",
            "Epoch   14/20 hypothesis: tensor([152.7606, 183.6184, 180.9164, 197.0138, 140.0571]) Cost: 1.625924\n",
            "Epoch   15/20 hypothesis: tensor([152.7802, 183.6427, 180.9399, 197.0395, 140.0759]) Cost: 1.623420\n",
            "Epoch   16/20 hypothesis: tensor([152.7909, 183.6565, 180.9530, 197.0538, 140.0865]) Cost: 1.622152\n",
            "Epoch   17/20 hypothesis: tensor([152.7968, 183.6643, 180.9603, 197.0618, 140.0927]) Cost: 1.621262\n",
            "Epoch   18/20 hypothesis: tensor([152.7999, 183.6688, 180.9644, 197.0661, 140.0963]) Cost: 1.620501\n",
            "Epoch   19/20 hypothesis: tensor([152.8014, 183.6715, 180.9665, 197.0686, 140.0985]) Cost: 1.619757\n",
            "Epoch   20/20 hypothesis: tensor([152.8020, 183.6731, 180.9677, 197.0699, 140.0999]) Cost: 1.619046\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du_eH0tdNLYI"
      },
      "source": [
        "## nn.Module 로 구현하는 선형 회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vA5Uxv65CrFS"
      },
      "source": [
        "이전까지는 선형 회귀를 좀 더 직접적으로 이해하기 위해 가설, 비용 함수를 직접 정의해서 선형 회귀 모델을 구현했다. 이번에는 파이토치에서 이미 구현되어 제공되고 있는 함수들을 불러 오는 것으로 더 쉽게 선형 회귀 모델을 구현\r\n",
        "\r\n",
        "ex, 파이토치에서는 선형 회귀 모델이 nn.Linear() 라는 함수로, 또 평균제곱오차가 nn.functional.mse_loss() 라는 함수로 구현되어져 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0X0twCu3C4sC"
      },
      "source": [
        "### 2.4.1. 단순 선형 회귀 구현\r\n",
        "\r\n",
        "데이터를 선언. 아래 데이터는  y= 2x 를 가정된 상태에서 만들어진 데이터로 우리는 이미 정답이 W=2, b=0 이라는 사실을 알고 있는 상태. 모델이 이  두 W와 b의 값을 제대로 맞히도록 하는 것이 목표"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMmqC2fkDBQd"
      },
      "source": [
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])\r\n",
        "#data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emCn3QhgDKbK"
      },
      "source": [
        "# 선형 회귀 모델 구현. nn.Linear()는 입력의 차원, 출력의 차원을 인수로 받는다.\r\n",
        "# 모델을 선언 및 초기화, 단순 선형 회귀이므로 input_dim=1, output_dim = 1\r\n",
        "\r\n",
        "model = nn.Linear(1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfCQEJPCDdXQ"
      },
      "source": [
        "model 에는 가중치 W와 편향 b가 저장되어 있는데 이 값은 model.parameters() 라는 함수를 사용하여 불러 올 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpDT7ZdiDwX9",
        "outputId": "ff7c53f7-5a5f-441a-d42d-ed4376f19046"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.5047]], requires_grad=True), Parameter containing:\n",
            "tensor([0.4902], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj65W-EFDy8i"
      },
      "source": [
        "2개의 값이 출력되는데 첫번 째 값이 W고 두 번째 값이 b 이다. 두 값 모두 현재는 랜덤 초기화가 되어 있는데 두 값 모두 학습의 대상이므로 requires_grad = True 가 돼있다.\r\n",
        "이제 옵티마이저를 정의한다. model.parameters()를 사용하여 W와 b를 전달한다. lr = 0.01로 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAOczXgcEENO"
      },
      "source": [
        "# optimizer 설정. 경사 하강법 SGD 를 사용하고, lr를 0.01\r\n",
        "\r\n",
        "optimizer= torch.optim.SGD(model.parameters(), lr= 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32iUn-NfELWP",
        "outputId": "59ed9d4d-efb4-4254-b513-f650c2556869"
      },
      "source": [
        "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\r\n",
        "\r\n",
        "nb_epochs = 2000\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "\r\n",
        "  #H(x) 계산\r\n",
        "  prediction = model(x_train)\r\n",
        "\r\n",
        "  # cost 계산\r\n",
        "\r\n",
        "  cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균제곱 오차 함수\r\n",
        "\r\n",
        "  #cost로 H(x) 개선\r\n",
        "  # gradient 를 0으로 초기화\r\n",
        "\r\n",
        "  optimizer.zero_grad()\r\n",
        "  # 비용 함수를 미분하여 gradient 계산\r\n",
        "  cost.backward() # backward 연산\r\n",
        "  # W와 b를 업데이트\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "  if epoch %100 == 0 :\r\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\r\n",
        "          epoch, nb_epochs, cost.item()\r\n",
        "      ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 7.743174\n",
            "Epoch  100/2000 Cost: 0.082010\n",
            "Epoch  200/2000 Cost: 0.050677\n",
            "Epoch  300/2000 Cost: 0.031315\n",
            "Epoch  400/2000 Cost: 0.019351\n",
            "Epoch  500/2000 Cost: 0.011958\n",
            "Epoch  600/2000 Cost: 0.007389\n",
            "Epoch  700/2000 Cost: 0.004566\n",
            "Epoch  800/2000 Cost: 0.002822\n",
            "Epoch  900/2000 Cost: 0.001744\n",
            "Epoch 1000/2000 Cost: 0.001077\n",
            "Epoch 1100/2000 Cost: 0.000666\n",
            "Epoch 1200/2000 Cost: 0.000411\n",
            "Epoch 1300/2000 Cost: 0.000254\n",
            "Epoch 1400/2000 Cost: 0.000157\n",
            "Epoch 1500/2000 Cost: 0.000097\n",
            "Epoch 1600/2000 Cost: 0.000060\n",
            "Epoch 1700/2000 Cost: 0.000037\n",
            "Epoch 1800/2000 Cost: 0.000023\n",
            "Epoch 1900/2000 Cost: 0.000014\n",
            "Epoch 2000/2000 Cost: 0.000009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cgfEF2lEq24"
      },
      "source": [
        "학습이 완료. cost의 값이 매우 작다. W와 b의 값도 보자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OoyKJ5BEEwHA",
        "outputId": "3bb51634-8fab-4fde-cdcd-c0dda5a80787"
      },
      "source": [
        "# 임의의 입력 4를 선언\r\n",
        "new_var = torch.FloatTensor([[4.0]])\r\n",
        "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\r\n",
        "pred_y = model(new_var) # forward 연산\r\n",
        "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\r\n",
        "print('훈련 후 입력이 4일 때의 예측값 :', pred_y)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9941]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecsExHsMFEf_"
      },
      "source": [
        "이 문제의 정답은 W가 2이므로 어느정도 최적화가 됐다 볼 수 있다. 실제 예측값은 8에 매우 가깝다 이제 학습 후의 W와 b를 출력해보겠"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbWKWJgqFQX2",
        "outputId": "9a83a3ce-6422-4a26-c236-78ae5cb5dbfa"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[1.9966]], requires_grad=True), Parameter containing:\n",
            "tensor([0.0078], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psmGAAW1FSZL"
      },
      "source": [
        "W 의 값이 2에 가깝고, b의 값이 0에 가까운 것을 볼 수 있다.\r\n",
        "- H(x) 식에 입력 x로부터 예측된 y를 얻는 것을 forward 연산이라고 한다.\r\n",
        "- 학습 전 , prediction = model(x_train)은 x_train 으로부터 예측값을 리턴하므로 forward 연산이다.\r\n",
        "- 학습 후 ,pred_y = model(new_var)는 임의의 값 new_var로부터 예측값을 리턴하므로 forward 연산.\r\n",
        "- 학습과정에서 비용 함수를 미분하여 기울기를 구하는 것을 backward 연산이라고 한다.\r\n",
        "- cost.backward() 는 비용 함수로부터 기울기를 구하라는 의미이며 backward 연산이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ5stcFEFwvo"
      },
      "source": [
        "### 2.4.2 다중 선형 회귀 구현하기\r\n",
        "\r\n",
        "이제 nn.Linear()와 nn.functional.mse_loss() 로 다중 선형 회귀를 구현해보자. 사실 코드 자체는 달라진 게 거의 없는데, nn.Linear() 의 인자값과 lr만 조절했다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUn4GQZJFwER"
      },
      "source": [
        "데이터를 선언한다. 3개의 x로부터 하나의 y를 예측하는 문제이다. 가설 수식은 $H(x) = w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} + b$입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Lliet4aGHWq"
      },
      "source": [
        "x_train = torch.FloatTensor([[73,80,75],\r\n",
        "                             [93, 88, 93],\r\n",
        "                            [89,91, 90],\r\n",
        "                            [96,98,100],\r\n",
        "                            [73,66,70]])\r\n",
        "y_train = torch.FloatTensor([[152],[185],[180],[196],[142]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qoXoBOnGXwE"
      },
      "source": [
        "# 선형 회귀 구현"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IGJ-mzpGfhv"
      },
      "source": [
        "# 모델을 선언 및 초기화, 다중 선형 회귀이므로 input_dim =3 , output_dim = 1\r\n",
        "model = nn.Linear(3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCnLoJMiGljK",
        "outputId": "06053efe-7714-465c-8234-5ec2821809ac"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[-0.3571,  0.5140, -0.2947]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.5325], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihbwU6wTHk96"
      },
      "source": [
        "# 첫번 째는 W의 값들, 두 번째는 b의 값, 현재는 모두 랜덤 초기화 된 상태. requires = True, 학습의 대상이라는 것\r\n",
        "\r\n",
        "# 옵티마이저 정의. model.parameters 를 사용하여 3개의 w와 b를 전달한다. lr = 0.00001 로 정의한다."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJOMgLzDHzSr"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oIKxBKrH4Sg",
        "outputId": "2cfa7a46-de76-48c0-edc9-826d69077c04"
      },
      "source": [
        "nb_epochs = 2000\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "  # H(x) 계산\r\n",
        "  prediction = model(x_train)\r\n",
        "  #model(x_train) = model.forward(x_train)\r\n",
        "\r\n",
        "  # cost 계산\r\n",
        "  cost= F.mse_loss(prediction, y_train)\r\n",
        "\r\n",
        "  # cost 로 H(x) 개선\r\n",
        "  # gradient 를 0으로 초기화\r\n",
        "  optimizer.zero_grad()\r\n",
        "  # 비용 함수를 미분하여 gradient 계산\r\n",
        "  cost.backward()\r\n",
        "  optimizer.step() # W와 b를 업데이트\r\n",
        "\r\n",
        "  if epoch % 100 ==0:\r\n",
        "    # 100번마다 로그 출력\r\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\r\n",
        "          epoch, nb_epochs, cost.item()\r\n",
        "      ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 34178.914062\n",
            "Epoch  100/2000 Cost: 12.461840\n",
            "Epoch  200/2000 Cost: 11.817289\n",
            "Epoch  300/2000 Cost: 11.206689\n",
            "Epoch  400/2000 Cost: 10.628275\n",
            "Epoch  500/2000 Cost: 10.080351\n",
            "Epoch  600/2000 Cost: 9.561388\n",
            "Epoch  700/2000 Cost: 9.069703\n",
            "Epoch  800/2000 Cost: 8.604002\n",
            "Epoch  900/2000 Cost: 8.162824\n",
            "Epoch 1000/2000 Cost: 7.744920\n",
            "Epoch 1100/2000 Cost: 7.349029\n",
            "Epoch 1200/2000 Cost: 6.974030\n",
            "Epoch 1300/2000 Cost: 6.618781\n",
            "Epoch 1400/2000 Cost: 6.282284\n",
            "Epoch 1500/2000 Cost: 5.963500\n",
            "Epoch 1600/2000 Cost: 5.661542\n",
            "Epoch 1700/2000 Cost: 5.375480\n",
            "Epoch 1800/2000 Cost: 5.104517\n",
            "Epoch 1900/2000 Cost: 4.847822\n",
            "Epoch 2000/2000 Cost: 4.604651\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4VBUwT8IWDy"
      },
      "source": [
        "# 학습 완료, Cost 의 값이 매우 작다. 3개의 W와 b도 확인해보자. x에 임의의 값 [73,80,75]를 넣어 모델이 예측하는 y의 값을 보자"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzS8ns2vIgJB",
        "outputId": "3ed59373-3c5f-4a35-af8c-a5797c90cc28"
      },
      "source": [
        "#임의의 입력 선언\r\n",
        "new_var = torch.FloatTensor([[73,80,75]])\r\n",
        "\r\n",
        "# 입력한 값 [73,80,75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\r\n",
        "pred_y = model(new_var)\r\n",
        "print('훈련 후 입력이 73, 80, 75일 때의 예측값: ' , pred_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 후 입력이 73, 80, 75일 때의 예측값:  tensor([[154.2830]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7mswYHiJVc6",
        "outputId": "f347af93-a3d6-48db-8a7c-b80b70d078b0"
      },
      "source": [
        "print(list(model.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parameter containing:\n",
            "tensor([[0.6169, 0.9175, 0.4849]], requires_grad=True), Parameter containing:\n",
            "tensor([-0.5207], requires_grad=True)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptkY2xmqJZ3E"
      },
      "source": [
        "## 2.5 클래스로 파이토치 모델 구현하기\r\n",
        "\r\n",
        "파이토치의 대부분 구현체들은 대부분 모델을 생성할 때 클래스(Class)를 사용하고 있다. 앞서 배운 선형 회귀를 ㄱ클래스로 구현한다. 앞서 구현한 코드와 다른 점은 오직 '클래스'로 모델을 구현했다는 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOD4WQ6kJmDl"
      },
      "source": [
        "### 2.5.1. 모델을 클래스로 구현하기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEGQ8ZeiJqt7"
      },
      "source": [
        "# 모델을 선언 및 초기화. 단순 선형 회귀이므로, input_dim = 1, output_dim = 1\r\n",
        "model = nn.Linear(1,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5sErNKoKF2G"
      },
      "source": [
        "class LinearRegressionModel(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.linear = nn.Linear(1,1)\r\n",
        "\r\n",
        "  def forward(self,x):\r\n",
        "    return self.linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc8dIX8cKSKC"
      },
      "source": [
        "model = LinearRegressionModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71nhO5nAKW8L"
      },
      "source": [
        "위와 같은 클래스를 사용한 모델 구현 형식은 대부분의 파이토치 구현체에서 사용하고 있는 방식으로 반드시 숙지해야 한다.\r\n",
        "\r\n",
        "클래스 형태의 모델을 nn.Module를 상속받는다. 그리고 init() 에서 모델의 구조와 동적을 정의하는 생성자를 정의한다. 이는 파이썬에서 객체가 갖는 속성값을 초기화하는 역할로, 객체가 생성될 때 자동으로 호출된다.\r\n",
        "\r\n",
        "super() 함수를 부르면 여기서 만든 클래스는 nn.Module 클래스의 속성들을 가지고 초기화된다.\r\n",
        "\r\n",
        "forward() 함수는 모델이 학습뎉이터를 입력받아 forward 연산을 진행시키는 함수\r\n",
        "이 함수는 model 객체를 데이터와 함께 호출하면 자동으로 실행된다. 예를 들어 model 이란 이름의 이름의 객체를 생성 후 ,model(입력 데이터)와 같은 형식으로 객체를 호출하면 자동으로 forward 연산이 수행된다.\r\n",
        "\r\n",
        "*   $H(x)$  식에 입력 $x$로부터 예측된 $y$를 얻는 것을 forward 연산이라고 합니다.\r\n",
        "\r\n",
        "앞어 다중 선형 회귀 모델은 다음과 같이 구"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6jsa91PK8mc"
      },
      "source": [
        "# 모델을 선언 및 초기화. 다중 선형 회귀이므로 input_dim = 3 output_dim =1 \r\n",
        "\r\n",
        "model = nn.Linear(3,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jESa534KLizW"
      },
      "source": [
        "class MultivariateLinearRegressionModel(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.linear = nn.Linear(3,1)\r\n",
        "\r\n",
        "    def forward(self,x):\r\n",
        "      return self.linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lq8pQ47LwjT"
      },
      "source": [
        "model = MultivariateLinearRegressionModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYWGr9ThLyiX"
      },
      "source": [
        "### 2.5.2 단순 선형 회귀 클래스로 구현하기\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dk5YkQ_tL2us"
      },
      "source": [
        "# 데이터\r\n",
        "\r\n",
        "x_train = torch.FloatTensor([[1],[2],[3]])\r\n",
        "y_train = torch.FloatTensor([[2],[4],[6]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LurNvLvJL-C5"
      },
      "source": [
        "class LinearRegressionModel(nn.Module):\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.linear = nn.Linear(1,1)\r\n",
        "\r\n",
        "  def forward(self,x):\r\n",
        "    return self.linear(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0k710UuMEaR"
      },
      "source": [
        "model = LinearRegressionModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LBGoSIbMHHe"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl7n3S_bMLVG",
        "outputId": "09c27442-67dd-423d-e369-1db75ef37995"
      },
      "source": [
        "nb_epochs = 2000\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "\r\n",
        "  #H(x)\r\n",
        "  prediction = model(x_train)\r\n",
        "\r\n",
        "  # cost\r\n",
        "  cost = F.mse_loss(prediction, y_train)\r\n",
        "\r\n",
        "  # cost 로 H(x) 개선\r\n",
        "\r\n",
        "  optimizer.zero_grad()\r\n",
        "\r\n",
        "  cost.backward()\r\n",
        "\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "  if epoch %100 == 0:\r\n",
        "    print('Epoch {:4d}/{} Cost: {:.6f}'.format(\r\n",
        "          epoch, nb_epochs, cost.item()\r\n",
        "      ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 13.372701\n",
            "Epoch  100/2000 Cost: 0.001315\n",
            "Epoch  200/2000 Cost: 0.000812\n",
            "Epoch  300/2000 Cost: 0.000502\n",
            "Epoch  400/2000 Cost: 0.000310\n",
            "Epoch  500/2000 Cost: 0.000192\n",
            "Epoch  600/2000 Cost: 0.000118\n",
            "Epoch  700/2000 Cost: 0.000073\n",
            "Epoch  800/2000 Cost: 0.000045\n",
            "Epoch  900/2000 Cost: 0.000028\n",
            "Epoch 1000/2000 Cost: 0.000017\n",
            "Epoch 1100/2000 Cost: 0.000011\n",
            "Epoch 1200/2000 Cost: 0.000007\n",
            "Epoch 1300/2000 Cost: 0.000004\n",
            "Epoch 1400/2000 Cost: 0.000003\n",
            "Epoch 1500/2000 Cost: 0.000002\n",
            "Epoch 1600/2000 Cost: 0.000001\n",
            "Epoch 1700/2000 Cost: 0.000001\n",
            "Epoch 1800/2000 Cost: 0.000000\n",
            "Epoch 1900/2000 Cost: 0.000000\n",
            "Epoch 2000/2000 Cost: 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9_2Du3aMdsx",
        "outputId": "77320a4b-9ed2-4c5a-a368-343a65996ce4"
      },
      "source": [
        "#다중 선형회귀 클래스로 구현하기\r\n",
        "\r\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\r\n",
        "                             [93, 88, 93],\r\n",
        "                             [89, 91, 90],\r\n",
        "                             [96, 98, 100],\r\n",
        "                             [73, 66, 70]])\r\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "class MultivariateLinearRegressionModel(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super().__init__()\r\n",
        "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return self.linear(x)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "model = MultivariateLinearRegressionModel()\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) \r\n",
        "\r\n",
        "nb_epochs = 2000\r\n",
        "for epoch in range(nb_epochs+1):\r\n",
        "\r\n",
        "    # H(x) 계산\r\n",
        "    prediction = model(x_train)\r\n",
        "    # model(x_train)은 model.forward(x_train)와 동일함.\r\n",
        "\r\n",
        "    # cost 계산\r\n",
        "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\r\n",
        "\r\n",
        "    # cost로 H(x) 개선하는 부분\r\n",
        "    # gradient를 0으로 초기화\r\n",
        "    optimizer.zero_grad()\r\n",
        "    # 비용 함수를 미분하여 gradient 계산\r\n",
        "    cost.backward()\r\n",
        "    # W와 b를 업데이트\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    if epoch % 100 == 0:\r\n",
        "    # 100번마다 로그 출력\r\n",
        "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\r\n",
        "          epoch, nb_epochs, cost.item()\r\n",
        "      ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/2000 Cost: 34730.640625\n",
            "Epoch  100/2000 Cost: 0.382716\n",
            "Epoch  200/2000 Cost: 0.373201\n",
            "Epoch  300/2000 Cost: 0.364163\n",
            "Epoch  400/2000 Cost: 0.355610\n",
            "Epoch  500/2000 Cost: 0.347489\n",
            "Epoch  600/2000 Cost: 0.339785\n",
            "Epoch  700/2000 Cost: 0.332485\n",
            "Epoch  800/2000 Cost: 0.325561\n",
            "Epoch  900/2000 Cost: 0.318991\n",
            "Epoch 1000/2000 Cost: 0.312752\n",
            "Epoch 1100/2000 Cost: 0.306845\n",
            "Epoch 1200/2000 Cost: 0.301232\n",
            "Epoch 1300/2000 Cost: 0.295918\n",
            "Epoch 1400/2000 Cost: 0.290865\n",
            "Epoch 1500/2000 Cost: 0.286075\n",
            "Epoch 1600/2000 Cost: 0.281538\n",
            "Epoch 1700/2000 Cost: 0.277212\n",
            "Epoch 1800/2000 Cost: 0.273119\n",
            "Epoch 1900/2000 Cost: 0.269227\n",
            "Epoch 2000/2000 Cost: 0.265536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW-SgvwdNMfZ"
      },
      "source": [
        "## 2.6 미니 배치와 데이터 로드 \r\n",
        "\r\n",
        "이번 채벝에서 배우는 내용은 선형 회귀에 한정되는 내용은 아니고 데이터를 로드하는 방법과 미니 배치 경사 하강법에 대해 학습한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh8xiMBONUKE"
      },
      "source": [
        "### 2.6.1 미니 배치와 배치 크기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kYeo-EhMZJT"
      },
      "source": [
        "# 앞서 다룬 데이터를 상기\r\n",
        "x_train = torch.FloatTensor([[73, 80, 75],\r\n",
        "                             [93, 88, 93],\r\n",
        "                             [89, 91, 90],\r\n",
        "                             [96, 98, 100],\r\n",
        "                             [73, 66, 70]])\r\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAGyE94nNaqg"
      },
      "source": [
        "위 데이터의 샘플의 개수는 5개이다. 전체 데이터를하나의 행렬로 선언하여 전체 데이터에 대해서 경사 하강법을 수행하여 학습할 수 있다. 근데 위 데이터는 현업에서 다루게 되는 방대한 데이터에 비하면 굉장히 적은 양이다. 만약 데이터가 수십만개 이상이면 전체 데이터에 대해 경사 하강법 하는 건 너무 오래 걸리고 많은 계산량이 필요하다.\r\n",
        "\r\n",
        "그래서 전체 데이터를 더 작은 단위로 나눠 해당 단위로 학습하게 하는 개념이 나오게 됐는데, 이를 미니배치 라고 한다.\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/55580/%EB%AF%B8%EB%8B%88%EB%B0%B0%EC%B9%98.PNG)\r\n",
        "\r\n",
        "위의 그림은 전체 데이터를 미니 배치 단위로 나눈 것을 보여준다. 미니 배치 학습을 하게되면 미니 배치만큼만 가져가서 미니 배치에 대한 비용을 계산하고, 경사 하강법을 수행한다. 그리고 다음 미니 배치를 가져가서 경사 하강법을 수행하고 마지막 미니 배치까지 이를 반복한다. 이렇게 전체 데이터에 대한 학습이 1회 끝나면 1에포크가 끝난다.\r\n",
        "- 에포크(epoch)는 전체 훈련 데이터가 학습에 한 번 사용된 주기를 의미한다.\r\n",
        "\r\n",
        "미니 배치 학습에서는 미니 배치의 개수만큼 경사 하강법을 수행해야 전체 데이터가 한 번 전부 사용되어 1 에포크가 된다. 미니 배치의 개수는 결국 미니 배치의 '크기를 몇으로 하느냐에 달렸느넫, 미니 배치의 크기를 batch_size 라고 한다.\r\n",
        "- 전체 데이터에 대해서 한 번에 경사 하강법을 수행하는 방법을 ' 배치 경사 하강법' 이라고 부른다. 반면 , 미니 배치 단위로 경사 하강법을 수행하는 것을' 미니 배치 경사 하강법' 이라고 부른다.\r\n",
        "- 배치 경사 하강법은 전체 데이터를 사용하므로 가중치 값이 최적값에 수렴하는 과정이 매우 안정적이지만, 계산량이 너무 많다. 미니 배치 경사 하강법은, 일부만을 보고 수행하므로 최적값으로 수렴하는 과정에서 값이 조금 헤매기도 하지만 훈련 속도가 빠르다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BiD28jEP4mm"
      },
      "source": [
        "### 2.6.2 이터레이션 (Iteration)\r\n",
        "\r\n",
        "![대체 텍스트](https://wikidocs.net/images/page/36033/batchandepochiteration.PNG)\r\n",
        "\r\n",
        "위 그림은 에포크와 배치 크기와 이터레이션의 관계를 보여준다.\r\n",
        "\r\n",
        "이터레이션은 한 번의 에포크 내에서 이루어지는 매개변수인 가중치 W와 b의 업데이트 횟수다. 전체 데이터가 2,000일 때 배치 크기를 200으로 하면 이터레이션은 10개이다. 에포크 당 업데이트가 10번 이루어짐을 의미한다.\r\n",
        "\r\n",
        "이제 미니 배치 학습을 할 수 있도록 해주는 파이토치의 도구들을 알아보자"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5e8N7LaVQSOg"
      },
      "source": [
        "### 2.6.3 데이터 로드"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-NqG4GNQWgw"
      },
      "source": [
        "파이토치에서는 데이터를 좀 더 쉽게 다룰 수 있도록 유용 한 도구로서 데이터셋과 데이터로더를 제공한다. 이를 사용하면 미니 배치 학습, 데이터 셔플, 병렬 처리까지 간단히 수행할 수 있다. 기본적인 사용 방법은 Dataset를 정의하고, 이를 DataLoader에 전달하는 것이다.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wi6l9tJQg5H"
      },
      "source": [
        "from torch.utils.data import TensorDataset # 텐서데이터셋\r\n",
        "from torch.utils.data import DataLoader # 데이터로더"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RXUt6yOiQh1b"
      },
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \r\n",
        "                               [93,  88,  93], \r\n",
        "                               [89,  91,  90], \r\n",
        "                               [96,  98,  100],   \r\n",
        "                               [73,  66,  70]])  \r\n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHHViljWQT5v"
      },
      "source": [
        "# 이제 이를 TensorDataset 의 입력으로 사용하고 dataset으로 저장"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo3GN35xRWbp"
      },
      "source": [
        "dataset = TensorDataset(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9AwM26gRZbs"
      },
      "source": [
        "파이토치의 데이터셋을 만들었다면 데이터로더를 사용 가능하다. 데이터로더는 기본적으로 2개의 인자를 입력받는데, 하나의 데이터셋, 미니 배치의 크기다. 이때 미니 배치의 크기는 통상적으로 2의 배수를 사용한다. 그리고 추가적으로 많이 사용되는 인자로 shuffle이 있다. shuffle = True 를 선택하면 Epoch 마다 데이터셋을 섞어서 데이터가 학습되는 순서를 바꾼다.\r\n",
        "\r\n",
        "사람도 같은 문제지를 계속 풀면 어느 순간 문제의 순서에 익숙해질 수 있다. 예를 들어 어떤 문제지의 12번 문제를 풀면서, '13번 문제가 뭔지는 기억은 안 나지만 어제 풀었ㅅ던 기억으로 5번이었떤 것 같은데' 하면서 문제 자체보단 순서에 익숙해질 수 있다. 그럴 때 문제지를 풀 때마다 순서를 랜덤으로 바꾸면 도움이 될 것이다. 마찬가지로 모델이 데이터셋의 순서에 익숙해지는 것을 방지하여 학습할 때는 이 옵션을 True로 주는 것을 권장한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3DK303fSLwC"
      },
      "source": [
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF8_4K6lSQEB"
      },
      "source": [
        "model = nn.Linear(3,1)\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19Ni2dtySVMA"
      },
      "source": [
        "이제 훈련을 진행한다. 아래 코드에서는 batch_idx 와 samples 를 주석 처리했는데 어떤 식으로 훈련되고 있는지 궁금하다면 주석 처리를 해제하고 훈련시켜보기 바람."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRguFcJyScdk",
        "outputId": "f69db4da-324f-42be-88fd-49147c65850f"
      },
      "source": [
        "nb_epochs = 20\r\n",
        "for epoch in range(nb_epochs + 1):\r\n",
        "  for batch_idx, samples in enumerate(dataloader):\r\n",
        "    # print(batch_idx)\r\n",
        "    # print(samples)\r\n",
        "    x_train, y_train = samples\r\n",
        "    # H(x) 계산\r\n",
        "    prediction = model(x_train)\r\n",
        "\r\n",
        "    # cost 계산\r\n",
        "    cost = F.mse_loss(prediction, y_train)\r\n",
        "\r\n",
        "    # cost로 H(x) 계산\r\n",
        "    optimizer.zero_grad()\r\n",
        "    cost.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\r\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\r\n",
        "        cost.item()\r\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Batch 1/3 Cost: 10411.642578\n",
            "Epoch    0/20 Batch 2/3 Cost: 4858.296875\n",
            "Epoch    0/20 Batch 3/3 Cost: 696.264954\n",
            "Epoch    1/20 Batch 1/3 Cost: 302.164368\n",
            "Epoch    1/20 Batch 2/3 Cost: 184.705536\n",
            "Epoch    1/20 Batch 3/3 Cost: 53.354000\n",
            "Epoch    2/20 Batch 1/3 Cost: 14.083602\n",
            "Epoch    2/20 Batch 2/3 Cost: 7.733485\n",
            "Epoch    2/20 Batch 3/3 Cost: 0.321681\n",
            "Epoch    3/20 Batch 1/3 Cost: 4.963413\n",
            "Epoch    3/20 Batch 2/3 Cost: 0.807077\n",
            "Epoch    3/20 Batch 3/3 Cost: 12.778421\n",
            "Epoch    4/20 Batch 1/3 Cost: 4.659064\n",
            "Epoch    4/20 Batch 2/3 Cost: 3.110271\n",
            "Epoch    4/20 Batch 3/3 Cost: 4.507976\n",
            "Epoch    5/20 Batch 1/3 Cost: 3.470737\n",
            "Epoch    5/20 Batch 2/3 Cost: 5.871498\n",
            "Epoch    5/20 Batch 3/3 Cost: 2.471012\n",
            "Epoch    6/20 Batch 1/3 Cost: 6.114941\n",
            "Epoch    6/20 Batch 2/3 Cost: 3.034981\n",
            "Epoch    6/20 Batch 3/3 Cost: 3.528511\n",
            "Epoch    7/20 Batch 1/3 Cost: 3.834924\n",
            "Epoch    7/20 Batch 2/3 Cost: 5.315554\n",
            "Epoch    7/20 Batch 3/3 Cost: 3.515053\n",
            "Epoch    8/20 Batch 1/3 Cost: 6.080107\n",
            "Epoch    8/20 Batch 2/3 Cost: 3.817443\n",
            "Epoch    8/20 Batch 3/3 Cost: 1.508876\n",
            "Epoch    9/20 Batch 1/3 Cost: 3.735501\n",
            "Epoch    9/20 Batch 2/3 Cost: 5.917191\n",
            "Epoch    9/20 Batch 3/3 Cost: 1.679906\n",
            "Epoch   10/20 Batch 1/3 Cost: 8.855163\n",
            "Epoch   10/20 Batch 2/3 Cost: 5.241042\n",
            "Epoch   10/20 Batch 3/3 Cost: 0.738447\n",
            "Epoch   11/20 Batch 1/3 Cost: 4.046379\n",
            "Epoch   11/20 Batch 2/3 Cost: 5.340021\n",
            "Epoch   11/20 Batch 3/3 Cost: 3.389125\n",
            "Epoch   12/20 Batch 1/3 Cost: 6.589547\n",
            "Epoch   12/20 Batch 2/3 Cost: 1.017473\n",
            "Epoch   12/20 Batch 3/3 Cost: 8.061073\n",
            "Epoch   13/20 Batch 1/3 Cost: 3.951192\n",
            "Epoch   13/20 Batch 2/3 Cost: 7.817137\n",
            "Epoch   13/20 Batch 3/3 Cost: 2.492454\n",
            "Epoch   14/20 Batch 1/3 Cost: 3.810210\n",
            "Epoch   14/20 Batch 2/3 Cost: 5.270946\n",
            "Epoch   14/20 Batch 3/3 Cost: 3.445879\n",
            "Epoch   15/20 Batch 1/3 Cost: 5.939319\n",
            "Epoch   15/20 Batch 2/3 Cost: 3.137150\n",
            "Epoch   15/20 Batch 3/3 Cost: 3.311205\n",
            "Epoch   16/20 Batch 1/3 Cost: 4.413442\n",
            "Epoch   16/20 Batch 2/3 Cost: 5.391868\n",
            "Epoch   16/20 Batch 3/3 Cost: 1.512139\n",
            "Epoch   17/20 Batch 1/3 Cost: 4.214175\n",
            "Epoch   17/20 Batch 2/3 Cost: 1.214107\n",
            "Epoch   17/20 Batch 3/3 Cost: 12.029368\n",
            "Epoch   18/20 Batch 1/3 Cost: 3.535393\n",
            "Epoch   18/20 Batch 2/3 Cost: 8.058764\n",
            "Epoch   18/20 Batch 3/3 Cost: 5.937242\n",
            "Epoch   19/20 Batch 1/3 Cost: 3.740370\n",
            "Epoch   19/20 Batch 2/3 Cost: 5.109043\n",
            "Epoch   19/20 Batch 3/3 Cost: 1.919224\n",
            "Epoch   20/20 Batch 1/3 Cost: 1.278397\n",
            "Epoch   20/20 Batch 2/3 Cost: 6.677893\n",
            "Epoch   20/20 Batch 3/3 Cost: 5.666675\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foaoiFN6SoyN",
        "outputId": "b1e93836-6773-4073-d8f8-7a7981bb6707"
      },
      "source": [
        "# 임의의 입력 [73, 80, 75]를 선언\r\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \r\n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\r\n",
        "pred_y = model(new_var) \r\n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[154.4468]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RRLy6x9SwBE"
      },
      "source": [
        "## 2.7 커스텀 데이터셋(Custom Dataset)\r\n",
        "\r\n",
        "앞 내용을 복습해보자. 파이토치에서는 데이터셋을 좀 더 쉽게 다룰 수 있도록 유용한 도구로서 , torch.utils.data.Dataset과 torch.utils.data.DataLoader를  제공한다. 이를 사용하면 미니 배치 학습, 데이터 셔플(shuffle) , 병렬 처리까ㅣ지 간단히 수행할 수 있다. 기본적인 사용 방법은 Dataset을 정의하고, 이를 DataLoader 에 ㄷ전달하는 것이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob4DabkuTaHJ"
      },
      "source": [
        "### 2.7.1 커스텀 데이터셋(Custsom Dataset)\r\n",
        "그런데 torch.utils.data.Dataset을 상속받아 직접 커스텀 데이터셋을 만드는 경우도 있다. torch.utils.data.Datset은 파이토치에서 데이터셋을 제공하는 추상 클래스이다. Dataset을 상속받아 다음 메소드들을 오버라이드 하여 커스텀 데이터셋을 만들어보겠다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "br0dMVrATs5f"
      },
      "source": [
        "# 커스텀 데이터셋을 만들 때, 일단 가장 기본적인 뼈대는 아래와 같다. 여기서 필요한 기본적인 define 은 3개이다.\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PK-EABWT0nq"
      },
      "source": [
        "class CustomDataset(torch.utils.data.Dataset): \r\n",
        "  def __init__(self):return\r\n",
        "#  데이터셋의 전처리를 해주는 부분\r\n",
        "\r\n",
        "  def __len__(self):return\r\n",
        "#  데이터셋의 길이. 즉, 총 샘플의 수를 적어주는 부분\r\n",
        "\r\n",
        "  def __getitem__(self, idx):return\r\n",
        "#  데이터셋에서 특정 1개의 샘플을 가져오는 함수"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PuFuVimnT1_p"
      },
      "source": [
        "# len(dataset)을 했을 때 데이터셋의 크기를 return 할 len\r\n",
        "\r\n",
        "# dataset[i]를 했을 때 i번재 샘플을 가져오도록 하는 인덱싱을 위한 get_item"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5z2S9SvZUCup"
      },
      "source": [
        "### 2.7.2 커스텀 데이터으로 선형회귀 구현하기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcaTs4oJTtyy"
      },
      "source": [
        "from torch.utils.data import Dataset\r\n",
        "from torch.utils.data import DataLoader\r\n",
        "\r\n",
        "class CustomDataset(Dataset): \r\n",
        "  def __init__(self):\r\n",
        "    self.x_data = [[73, 80, 75],\r\n",
        "                   [93, 88, 93],\r\n",
        "                   [89, 91, 90],\r\n",
        "                   [96, 98, 100],\r\n",
        "                   [73, 66, 70]]\r\n",
        "    self.y_data = [[152], [185], [180], [196], [142]]\r\n",
        "\r\n",
        "  # 총 데이터의 개수를 리턴\r\n",
        "  def __len__(self): \r\n",
        "    return len(self.x_data)\r\n",
        "\r\n",
        "  # 인덱스를 입력받아 그에 맵핑되는 입출력 데이터를 파이토치의 Tensor 형태로 리턴\r\n",
        "  def __getitem__(self, idx): \r\n",
        "    x = torch.FloatTensor(self.x_data[idx])\r\n",
        "    y = torch.FloatTensor(self.y_data[idx])\r\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_o9t_mrCUPgl"
      },
      "source": [
        "dataset = CustomDataset()\r\n",
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_sCxLanUWcp"
      },
      "source": [
        "model = torch.nn.Linear(3,1)\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7M2A0uIUbaw",
        "outputId": "68c9378a-2699-41e6-8673-2ea0e4f474a8"
      },
      "source": [
        "nb_epochs = 20\r\n",
        "for epoch in range(nb_epochs + 1):\r\n",
        "  for batch_idx, samples in enumerate(dataloader):\r\n",
        "    # print(batch_idx)\r\n",
        "    # print(samples)\r\n",
        "    x_train, y_train = samples\r\n",
        "    # H(x) 계산\r\n",
        "    prediction = model(x_train)\r\n",
        "\r\n",
        "    # cost 계산\r\n",
        "    cost = F.mse_loss(prediction, y_train)\r\n",
        "\r\n",
        "    # cost로 H(x) 계산\r\n",
        "    optimizer.zero_grad()\r\n",
        "    cost.backward()\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    print('Epoch {:4d}/{} Batch {}/{} Cost: {:.6f}'.format(\r\n",
        "        epoch, nb_epochs, batch_idx+1, len(dataloader),\r\n",
        "        cost.item()\r\n",
        "        ))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0/20 Batch 1/3 Cost: 44199.386719\n",
            "Epoch    0/20 Batch 2/3 Cost: 18326.742188\n",
            "Epoch    0/20 Batch 3/3 Cost: 5618.166504\n",
            "Epoch    1/20 Batch 1/3 Cost: 1294.254883\n",
            "Epoch    1/20 Batch 2/3 Cost: 460.197510\n",
            "Epoch    1/20 Batch 3/3 Cost: 192.675720\n",
            "Epoch    2/20 Batch 1/3 Cost: 35.667679\n",
            "Epoch    2/20 Batch 2/3 Cost: 14.944163\n",
            "Epoch    2/20 Batch 3/3 Cost: 0.434156\n",
            "Epoch    3/20 Batch 1/3 Cost: 3.636916\n",
            "Epoch    3/20 Batch 2/3 Cost: 2.199007\n",
            "Epoch    3/20 Batch 3/3 Cost: 0.002766\n",
            "Epoch    4/20 Batch 1/3 Cost: 1.721153\n",
            "Epoch    4/20 Batch 2/3 Cost: 0.398790\n",
            "Epoch    4/20 Batch 3/3 Cost: 2.327561\n",
            "Epoch    5/20 Batch 1/3 Cost: 1.248785\n",
            "Epoch    5/20 Batch 2/3 Cost: 1.354533\n",
            "Epoch    5/20 Batch 3/3 Cost: 2.051692\n",
            "Epoch    6/20 Batch 1/3 Cost: 2.236589\n",
            "Epoch    6/20 Batch 2/3 Cost: 0.235299\n",
            "Epoch    6/20 Batch 3/3 Cost: 1.027871\n",
            "Epoch    7/20 Batch 1/3 Cost: 1.193879\n",
            "Epoch    7/20 Batch 2/3 Cost: 1.408593\n",
            "Epoch    7/20 Batch 3/3 Cost: 0.411535\n",
            "Epoch    8/20 Batch 1/3 Cost: 0.051273\n",
            "Epoch    8/20 Batch 2/3 Cost: 2.332812\n",
            "Epoch    8/20 Batch 3/3 Cost: 0.430304\n",
            "Epoch    9/20 Batch 1/3 Cost: 1.387053\n",
            "Epoch    9/20 Batch 2/3 Cost: 0.359174\n",
            "Epoch    9/20 Batch 3/3 Cost: 2.360032\n",
            "Epoch   10/20 Batch 1/3 Cost: 2.868891\n",
            "Epoch   10/20 Batch 2/3 Cost: 1.032753\n",
            "Epoch   10/20 Batch 3/3 Cost: 0.049780\n",
            "Epoch   11/20 Batch 1/3 Cost: 1.773907\n",
            "Epoch   11/20 Batch 2/3 Cost: 0.338609\n",
            "Epoch   11/20 Batch 3/3 Cost: 2.108514\n",
            "Epoch   12/20 Batch 1/3 Cost: 1.304955\n",
            "Epoch   12/20 Batch 2/3 Cost: 1.997677\n",
            "Epoch   12/20 Batch 3/3 Cost: 0.101927\n",
            "Epoch   13/20 Batch 1/3 Cost: 0.176272\n",
            "Epoch   13/20 Batch 2/3 Cost: 2.000881\n",
            "Epoch   13/20 Batch 3/3 Cost: 0.802782\n",
            "Epoch   14/20 Batch 1/3 Cost: 1.373983\n",
            "Epoch   14/20 Batch 2/3 Cost: 1.293265\n",
            "Epoch   14/20 Batch 3/3 Cost: 0.042666\n",
            "Epoch   15/20 Batch 1/3 Cost: 2.117120\n",
            "Epoch   15/20 Batch 2/3 Cost: 0.341467\n",
            "Epoch   15/20 Batch 3/3 Cost: 0.059716\n",
            "Epoch   16/20 Batch 1/3 Cost: 1.183155\n",
            "Epoch   16/20 Batch 2/3 Cost: 0.431431\n",
            "Epoch   16/20 Batch 3/3 Cost: 2.351320\n",
            "Epoch   17/20 Batch 1/3 Cost: 1.846995\n",
            "Epoch   17/20 Batch 2/3 Cost: 0.177135\n",
            "Epoch   17/20 Batch 3/3 Cost: 2.280913\n",
            "Epoch   18/20 Batch 1/3 Cost: 2.404896\n",
            "Epoch   18/20 Batch 2/3 Cost: 1.781771\n",
            "Epoch   18/20 Batch 3/3 Cost: 0.001449\n",
            "Epoch   19/20 Batch 1/3 Cost: 0.331640\n",
            "Epoch   19/20 Batch 2/3 Cost: 2.075177\n",
            "Epoch   19/20 Batch 3/3 Cost: 0.454867\n",
            "Epoch   20/20 Batch 1/3 Cost: 0.056907\n",
            "Epoch   20/20 Batch 2/3 Cost: 2.305078\n",
            "Epoch   20/20 Batch 3/3 Cost: 0.417488\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygArkv4NUcf7",
        "outputId": "9f19e14a-948d-4633-d290-ea1a9a268053"
      },
      "source": [
        "# 임의의 입력 [73, 80, 75]를 선언\r\n",
        "new_var =  torch.FloatTensor([[73, 80, 75]]) \r\n",
        "# 입력한 값 [73, 80, 75]에 대해서 예측값 y를 리턴받아서 pred_y에 저장\r\n",
        "pred_y = model(new_var) \r\n",
        "print(\"훈련 후 입력이 73, 80, 75일 때의 예측값 :\", pred_y) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "훈련 후 입력이 73, 80, 75일 때의 예측값 : tensor([[151.5672]], grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}